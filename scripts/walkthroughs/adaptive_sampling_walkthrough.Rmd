---
title: "Adaptive sampling walkthrough"
author: "TMM"
date: "2023-10-24"
output: html_document
---

```{r setup}

library(mgcv)
library(terra)
library(tidyverse)
library(tidyterra)
library(sf)
library(viridis)

source("../../scripts/functions/as_functions.R")

```


```{r initial_data}

# environmental data for raster
envdat <- rast('../../data/environmental_data_subset.tif')

# true species distribution
species_distrib_rast <- rast('../../outputs/simulated_data/true_species_distribution.tif')
true_species_distrib_df <- as.data.frame(species_distrib_rast, xy=TRUE)
colnames(true_species_distrib_df) <- c("x", "y", "presence")
plot(species_distrib_rast, main = "True species distribution")

```


```{r choose_nlocs_dist}

# choose number of new locations to sample and distance between them
nlocs <- 1200
dist_apart <- 10000

write = TRUE

```


# Introduction

This R notebook accompanies figure XX in the paper 'Adaptive sampling in ecology: current practice and future opportunities'. Here, we aim to demonstrate how adaptive sampling can be used in an ecological setting. For this, we take the case of species distribution models, where we are aiming to improve our knowledge of species' distributions.

This will follow several steps:

0. Generation of initial data
1. Defining a criterion
1a. Empirical
1b. Model-based
2. Selection of new sampling occasions
2a. Optimised
2b. Independent draws
3. Sampling activity
- Rounds vs batches


# Adaptive sampling

## Step 0. Generation of initial data

For any adaptive sampling programme, we first need some initial data on which we can base our further sampling. This is code is located in `/scripts/processing/simulate_species.R` and uses some environmental variables to generate the true distribution of a species. We then sample this true species distribution according to the suburban distribution of GB, mimicking real life where people tend to record close to their homes.

```{r load_species_data}

species_records <- read.csv("../../outputs/simulated_data/sampled_species_distrib.csv")[,-1]

# define observations
species_records$observations <- 1

### create GB outline
gboutline <- envdat[["elev"]]
values(gboutline)[!is.na(values(gboutline))] <- 1
gboutline <- as.polygons(gboutline)
# plot(gboutline)


initial_records_plot <- 
  ggplot() +
  geom_sf(data = gboutline) +
  geom_point(data = species_records, aes(x,y)) +
  coord_sf() +
  theme_void()
initial_records_plot

if(write){
  ggsave(initial_records_plot,
         file = "../../outputs/plots/initial_records.png",
         height = 5, width = 4)
}

```

## Step 1. Definition of a criterion

### Step 1a. Empirical criterion

An empirical criterion involves using only the data to determine locations for further sampling. In this case, we will identify regions that have not yet been sampled. In the code below, we convert our species records into a 1km binary raster format for presences (`1`) and absences (`0`). This creates a raster map of the species' distribution which will be our *empirical sampling layer*.


```{r empirical}

# convert species records to raster
species_rast <- rasterize(x = as.matrix(species_records[,1:2]), y = envdat[["elev"]], 
                          values = 1, background = 0)
species_rast_gb <- mask(species_rast, envdat[["elev"]]) # mask to GB

## visualise
species_sampled_distrib_df <- as.data.frame(species_rast_gb, xy = TRUE)
colnames(species_sampled_distrib_df) <- c('x', 'y', 'observations')

# plot
obssp_dsitrib <- ggplot(species_sampled_distrib_df, 
                        aes(x,y, fill = factor(observations))) +
  geom_raster() +
  scale_fill_manual(name = "", 
                    labels = c("No records", "Records"),
                    values = c("#E69F00", "#009E73")) +
  coord_sf(crs = 27700) +
  
  theme_void() +
  theme(text = element_text(size = 30))
obssp_dsitrib

if(write) {
  
  ggsave(obssp_dsitrib, 
         file = "../../outputs/plots/empirical_criterion.png",
         height = 6,width = 5)
  
}

# Unvisited locations are any 0s!
emp_criterion_absences <- species_sampled_distrib_df[species_sampled_distrib_df$observations == 0,]


```


### Step 1b. Model-based criterion

In a model-based adaptive sampling criterion, further sampling is based on some aspect of a model. Here, we will model the species distribution as a function of several environmental variables using GAMs. Then we will use the standard error as our measure of uncertainty on which to base further sampling. 

#### Pseudoabsences

A key consideration in SDM of presence-only data is the choice of method to get pseudoabsences. For simplicity, here we choose to use a random background approach as we are not interested in its effect on model performance. We will randomly sample the same number of points as there were presences from all of the locations which do not already have records.

```{r pseudoabsences}

# randomly sample
psedoabs_index <- sample(1:nrow(emp_criterion_absences), 
                         size = sum(species_sampled_distrib_df$observations==1))

# extract pseudoabsences
pseudoabs <- emp_criterion_absences[psedoabs_index,]

# bind to observations
speciesobs_df <- rbind(species_records, pseudoabs)

# visualise
ggplot(speciesobs_df, aes(x, y, col = factor(observations))) +
  geom_point(size = 0.2) +
  scale_colour_manual(name = '', labels = c("Pseudoabsences", "Presences"),
                      values = c("#E69F00", "#009E73")) +
  
  theme_bw()

```

#### Modelling

The presences and pseudoabsences are then used to run a binomial GAM. We model this with three explanatory variables, the percentage of improved grassland, the elevation and the annual mean temperature. From this model we can then plot the prediction and the standard error back onto GB. The standard error is the model-based sampling metric that we will use to determine subsequent sampling. Targetting regions of high model uncertainty could be valuable for improving model performance.   

```{r modelling}

## get environmental data for each observation -- extract() from envdat
# choose the variables used to simulate the species and a few others

# extract data
recsdf <- cbind(speciesobs_df, 
                terra::extract(envdat, speciesobs_df[1:2], 
                               xy = FALSE, method = "simple"))
# head(recsdf)

# model - simple model with few explanatory variables
gammod <- gam(observations ~ s(impr_grass) +
                # s(elev) +
                s(bio_1) +
                s(bio_12) +
                s(x,y),
              family = 'binomial',
              data = recsdf)
summary(gammod)
# plot(gammod)

# convert environmental raster to data frame
edatdf <- as.data.frame(envdat[[c("impr_grass", "elev", "bio_1", "bio_12")]],
                        na.rm = TRUE,
                        xy = TRUE)

# create a new x variable
newx <- edatdf[, c("x", "y", "impr_grass", "elev", "bio_1", "bio_12")]

# predict onto environment
newy <- predict(gammod, 
                newx,
                type = "response",
                se.fit = TRUE)

modelbased_criterion <- na.omit(cbind(newx, newy))


```


#### Extracting uncertainty

This is simply done by predicting the standard error for all of GB.

```{r standard error}

# probability of presences
ggplot(modelbased_criterion, aes(x, y, fill = fit)) +
  geom_tile() +
  scale_fill_viridis_c(name = "Probability of presence") +
  
  theme_bw()

# standard error 
model_based_crit_plot <- ggplot(modelbased_criterion, 
                                aes(x, y, fill = log(se.fit))) +
  geom_raster() +
  scale_fill_viridis(name = "Standard\nerror", 
                     begin = 0.1,
                     labels = c("Low", "High"),
                     breaks = c(-31, 0.120)) +
  coord_sf(crs = 27700) +
  
  theme_void() +
  theme(text = element_text(size = 30))
model_based_crit_plot

if(write) {
  
  ggsave(model_based_crit_plot, 
         file = "../../outputs/plots/modelbased_criterion.png",
         height = 6,width = 5)
  
}


```


## Step 2. Selection of new sampling occasions

Important to choose the number of new locations versus number of batches. We're going to choose `r print(nlocs)` observations

### Step 2a. Optimised

#### Empirical method - optimised

To do this, we will select a random location from all of the places that haven't already been visited (i.e. the empricial criterion). We will then remove all other unvisited locations within `r print(dist_apart)`m of it, and sample another point. This will be done until `r print(nlocs)` have been sampled.


```{r empirical_optimised}

# convert empirical absences to sf
emp_absences <- st_as_sf(emp_criterion_absences[,1:2], coords = c("x", "y"), crs = 27700)

# save uncertainty layer into a new file that can have points removed
emp_abs_sftoedit <- emp_absences
empabsoutdf <- data.frame() # data frame for storage

i <- 1
while(i<=nlocs) {
  
  # if(i%%50==0) print(i)
  
  # sample a random point from all absences
  sampled_point_ind <- sample(1:nrow(emp_abs_sftoedit), size = 1)
  sampled_point <- emp_abs_sftoedit[sampled_point_ind,]
  
  # store that point
  empabsoutdf <- rbind(empabsoutdf, sampled_point)
  
  # remove the highest point from the data frame
  emp_abs_sftoedit <- emp_abs_sftoedit[-sampled_point_ind,]
  
  # find all points within x distance of highest uncert
  absence_points_nearby <- unlist(st_is_within_distance(sampled_point, emp_abs_sftoedit,
                                                        dist = dist_apart))
  
  if(length(absence_points_nearby)>0){
    # remove those points from the absences dataframe 
    emp_abs_sftoedit <- emp_abs_sftoedit[-absence_points_nearby,]
  }
  
  i <- i+1
}

# plot
emp_optim <- ggplot() +
  geom_raster(data = species_sampled_distrib_df, aes(x,y, fill = factor(observations))) +
  scale_fill_manual(name = '', labels = c("Absences", "Observations"),
                    values = c("#E69F00", "#009E73")) +
  geom_sf(data = empabsoutdf, aes(colour = "New recording\nlocations")) +
  coord_sf(expand = FALSE) +
  scale_colour_manual(name = "", values = "black") +
  # ylim(0, 350000) +
  ylim(622500, 980000) +
  xlim(45000, 425000) +
  
  theme_void() +
  theme(text = element_text(size = 25)) +
  guides(fill = "none", colour = "none")
emp_optim

if(write) {
  
  ggsave(emp_optim, 
         file = "../../outputs/plots/emp_optim_selection.png",
         height = 5,width = 5)
  
}


```


#### Model-based - optimised

We will select the location that has the highest model uncertainty. We will then remove all other unvisited locations within `r print(dist_apart)`m of it. Then we sample the location with the next highest uncertainty and repeat until `r print(nlocs)` have been sampled.

```{r model-based_optimised}

# get only uncertainty and coordinates from model based criterion
modcrit_dfsub <- modelbased_criterion[,c("x", "y", "se.fit")]

# convert to sf
uncertsf <- st_as_sf(modcrit_dfsub, coords = c("x", "y"), crs = 27700)

# order the uncertainty layer according to decreasing uncertainty - se.fit
uncertsf <- uncertsf[order(uncertsf$se.fit,
                           decreasing = TRUE), ]

# save uncertainty layer into a new file that can have points removed
uncertsftoedit <- uncertsf
uncertoutdf <- data.frame() # data frame for storage

i <- 1
while(i<=nlocs) {
  
  # if(i%%50==0) print(i)
  
  # get the highest uncertainty point
  highest_uncert <- uncertsftoedit[1,]
  # print(highest_uncert)
  
  # store that point
  uncertoutdf <- rbind(uncertoutdf, highest_uncert)
  
  # remove the highest point from the data frame
  uncertsftoedit <- uncertsftoedit[-1,]
  
  # find all points within x distance of highest uncert
  uncert_points_nearby <- unlist(st_is_within_distance(highest_uncert, uncertsftoedit,
                                                       dist = dist_apart))
  
  if(length(uncert_points_nearby)>0){
    # remove those points from the uncertainty dataframe 
    uncertsftoedit <- uncertsftoedit[-uncert_points_nearby,]
  }
  
  i <- i+1
}

# plot
model_optim <- ggplot() +
  geom_raster(data = modelbased_criterion, aes(x, y, fill = log(se.fit))) +
  scale_fill_viridis_c(name = "Standard error", begin = 0.1) +
  geom_sf(data = uncertoutdf, aes(colour = "New recording\nlocations")) +
  coord_sf(expand = FALSE) +
  scale_colour_manual(name = "", values = "black") +
  # ylim(0, 350000) +
  ylim(622500, 980000) +
  xlim(45000, 425000) +
  
  theme_void() +
  theme(text = element_text(size = 25)) +
  guides(fill = "none", colour = "none")
model_optim

if(write) {
  
  ggsave(model_optim, 
         file = "../../outputs/plots/modelbased_optim_selection.png",
         height = 5,width = 5)
  
}

```


### Step 2b. Independent draws

#### Empirical independent draws

Randomly sample from all unvisited locations without replacement.

```{r empirical_independent_draws}

# unvisited areas
emp_critind <- sample(1:nrow(emp_criterion_absences), nlocs) ## how many new locations are visited

# New locations visited
emp_indepdraw_new_locs <- st_as_sf(emp_criterion_absences[emp_critind,], 
                                   coords = c("x", "y"), crs = 27700)

# plot
empindep_draws <- ggplot() +
  geom_raster(data = species_sampled_distrib_df, aes(x,y, fill = factor(observations))) +
  # geom_point(data = species_records, aes(x,y, colour = "original records"), size = 0.5) +
  geom_sf(data = emp_indepdraw_new_locs, 
          aes(colour = "New recording\nlocations")) +
  coord_sf(expand = FALSE) +
  scale_colour_manual(values = c("black"), name = "") +
  scale_fill_manual(name = '', labels = c("Absences", "Observations"),
                    values = c("#E69F00", "#009E73")) +
  # ylim(0, 350000) +
  ylim(622500, 980000) +
  xlim(45000, 425000) +
  
  theme_void() +
  theme(text = element_text(size = 25)) +
  guides(fill = "none", colour = "none")
empindep_draws

if(write) {
  
  ggsave(empindep_draws, 
         file = "../../outputs/plots/empirical_indep_selection.png",
         height = 5,width = 5)
  
}

```

#### Model-based indpendent draws

Select the 400 locations with the highest uncertainty

```{r model-based_independent_draws}

sorted_data <- modelbased_criterion[order(modelbased_criterion$se.fit,
                                          decreasing = TRUE), ]

top_uncert <- st_as_sf(sorted_data[1:nlocs, c("x", "y")], 
                       coords = c("x", "y"), crs = 27700)

# standard error + top error
model_indep <- ggplot() +
  geom_raster(data=modelbased_criterion, aes(x, y, fill = log(se.fit))) +
  geom_sf(data=top_uncert, aes(colour = "New recording\nlocations")) +
  # geom_sf(data = gboutline, fill = NA) +
  coord_sf(expand = FALSE) +
  scale_fill_viridis_c(name = "Standard error", begin = 0.1) +
  scale_colour_manual(name = "", values = "black") +
  # ylim(0, 350000) +
  ylim(622500, 980000) +
  xlim(45000, 425000) +
  
  theme_void() +
  theme(text = element_text(size = 25)) +
  guides(fill = "none", colour = "none")
model_indep

if(write) {
  
  ggsave(model_indep, 
         file = "../../outputs/plots/model_indep_selection.png",
         height = 5, width = 5)
  
}

```


### Step 3. Rounds vs batches

In adaptive sampling there will be a decision to make about the number of rounds of adaptive sampling and how many points to sample at each time point. For example, if data collection involved a lot of organisation and a trip overseas, it might be that it's only possible to undertake a single collection incident In other cases, it might be that we are able to carry out sampling over several iterations, with the criterion layer recalculated at each step.

Here we show how this is possible. 

```{r rounds_and_batches}

# Model rounds and batches
as_batches <- adaptive_sampling_rounds(nbatches = 4,
                                       total_number_locs = nlocs,
                                       
                                       # uncertainty layer from first model
                                       modelbased_crit_sf = uncertsf,
                                       
                                       # the initial species records
                                       # I could add batch to this so that we can account for batch number in the modelling?
                                       initial_species_records = species_records,
                                       
                                       # distance between records to maintain
                                       distance_apart = dist_apart)

```


```{r plotting batches}


## get initial uncertainty data bound to AS uncert

# get the initial uncertainty layer as dataframe
initial_uncert_data <- data.frame(x = st_coordinates(uncertsf)[,1],
                                  y = st_coordinates(uncertsf)[,2],
                                  se.fit = uncertsf$se.fit, 
                                  batch = 'initial')

# bind to the new uncertainty layers
all_uncert_data <- rbind(initial_uncert_data, as_batches[["model_uncertainty"]])

# remove batch #4 - no fifth round of adaptive sampling
all_uncert_data <- all_uncert_data[all_uncert_data$batch != 4,]

# convert the batch number to factor and rename
new_locations <- as_batches[["all_adaptive_locations"]]

new_locations$batch <- factor(new_locations$batch)
all_uncert_data$batch <- factor(all_uncert_data$batch, levels = c("initial", 1, 2, 3))

levels(new_locations$batch) <- c("Batch 1","Batch 2",
                                "Batch 3","Batch 4")
levels(all_uncert_data$batch) <- c("Batch 1","Batch 2",
                                   "Batch 3","Batch 4")



# plot
model_optim <- ggplot() +
  geom_raster(data = all_uncert_data, aes(x, y, fill = log(se.fit))) +
  scale_fill_viridis_c(name = "Standard error", begin = 0.1) +
  geom_point(data = new_locations, aes(x,y), size = 0.8) +#,colour = factor(observations))) +
  coord_sf(expand = FALSE) +
  # ylim(0, 350000) +
  ylim(622500, 980000) +
  xlim(45000, 425000) +
  
  theme_bw() +
  theme(text = element_text(size = 30),
        axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.grid.major = element_blank(),
        strip.text.x = element_text(size = 18, family="Calibri"),
        strip.background = element_blank()) +
  guides(fill = "none") +
  # scale_colour_manual(name = "Species\npresence", values = c("black", "grey")) +
  
  facet_wrap(~batch, ncol = 2)
model_optim


if(write) {
  
  ggsave(model_optim, 
         file = "../../outputs/plots/batch_adaptive_sampling.png",
         height = 6, width = 5)
  
}

```



```{r new records}

new_records_plot <- 
  ggplot() +
  geom_sf(data = gboutline) +
  geom_point(data = new_locations[new_locations$observations==1,], aes(x,y), colour="darkgreen") +
  coord_sf() +
  theme_void()
new_records_plot

if(write){
  ggsave(new_records_plot,
         file = "../../outputs/plots/new_records.png",
         height = 5, width = 4)
}

```



#### the easy way of showing batches

Only temporary - need to code the different batches and rerunning the model each time when I have the energy

```{r}

library(patchwork)

batches <- sample(1:4, size = nrow(uncertoutdf), replace = TRUE)

batched_data <- cbind(uncertoutdf, batches)

head(batched_data)

# plot
model_optim <- ggplot() +
  geom_raster(data = modelbased_criterion, aes(x, y, fill = se.fit)) +
  scale_fill_viridis_c(name = "Standard error", begin = 0.1) +
  geom_sf(data = batched_data, aes(colour = factor(batches))) +
  coord_sf(expand = FALSE) +
  scale_colour_manual(name = "", values = c("black", "white", "red", "orange")) +
  ylim(0, 350000) +
  
  theme_void() +
  theme(text = element_text(size = 25)) +
  guides(fill = "none", colour = "none") +
  facet_wrap(~batches, ncol = 2)
model_optim

```

